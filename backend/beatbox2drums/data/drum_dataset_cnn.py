#!/usr/bin/env python3
"""
PyTorch Dataset for CNN-detected onset spectrograms.

Loads data from .npz files generated by prepare_classifier_data_cnn.py
"""

import torch
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler
import numpy as np
from pathlib import Path
from collections import Counter


class DrumDatasetCNN(Dataset):
    """
    Dataset for loading preprocessed drum onset spectrograms from CNN detector.

    Loads from .npz files containing:
    - X: (n_samples, n_mels, window_frames) spectrograms
    - y: (n_samples,) string labels
    """

    # Class names and indices
    CLASS_NAMES = ['kick', 'snare', 'hihat']
    CLASS_TO_IDX = {'kick': 0, 'snare': 1, 'hihat': 2}
    IDX_TO_CLASS = {0: 'kick', 1: 'snare', 2: 'hihat'}

    def __init__(self, data_file, transform=None):
        """
        Args:
            data_file: Path to .npz file (e.g., train_data_cnn.npz)
            transform: Optional transform to apply to spectrograms
        """
        self.data_file = Path(data_file)
        self.transform = transform

        if not self.data_file.exists():
            raise ValueError(f"Data file not found: {self.data_file}")

        # Load data
        print(f"Loading data from {self.data_file}...")
        data = np.load(self.data_file)

        self.X = data['X']  # (n_samples, n_mels, window_frames)
        self.y_str = data['y']  # (n_samples,) string labels

        # Convert string labels to indices
        self.y = np.array([self.CLASS_TO_IDX[label] for label in self.y_str])

        # Compute class distribution
        self.class_counts = Counter(self.y)

        print(f"Loaded {self.data_file.stem}:")
        print(f"  Total samples: {len(self.X):,}")
        print(f"  Data shape: {self.X.shape}")
        for class_name in self.CLASS_NAMES:
            class_idx = self.CLASS_TO_IDX[class_name]
            count = self.class_counts[class_idx]
            pct = count / len(self.X) * 100
            print(f"    {class_name}: {count:,} ({pct:.1f}%)")

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        """
        Get a single sample.

        Returns:
            Tuple of (spectrogram, label)
            - spectrogram: (1, n_mels, window_frames) tensor
            - label: int class index
        """
        # Get spectrogram (n_mels, window_frames)
        spec = self.X[idx]

        # Add channel dimension: (n_mels, window_frames) → (1, n_mels, window_frames)
        spec = spec[np.newaxis, :, :]

        # Convert to tensor
        spec = torch.from_numpy(spec).float()

        # Apply transform if provided
        if self.transform is not None:
            spec = self.transform(spec)

        label = int(self.y[idx])

        return spec, label

    def get_class_weights(self):
        """
        Compute class weights for balanced loss.

        Returns:
            Tensor of shape (num_classes,) with class weights
        """
        total = len(self.X)
        weights = torch.zeros(len(self.CLASS_NAMES))

        for class_idx, count in self.class_counts.items():
            # Weight = total / (num_classes * count)
            weights[class_idx] = total / (len(self.CLASS_NAMES) * count)

        return weights

    def get_sample_weights(self):
        """
        Get per-sample weights for WeightedRandomSampler.

        Returns:
            List of weights, one per sample
        """
        class_weights = self.get_class_weights()
        sample_weights = [class_weights[label].item() for label in self.y]
        return sample_weights


def create_dataloaders_cnn(data_dir, batch_size=64, num_workers=4,
                            use_weighted_sampling=False, pin_memory=True):
    """
    Create train, val, and test dataloaders from CNN-based preprocessed data.

    Args:
        data_dir: Directory containing .npz files (train_data_cnn.npz, val_data_cnn.npz, etc.)
        batch_size: Batch size
        num_workers: Number of data loading workers
        use_weighted_sampling: Whether to use weighted sampling for training
        pin_memory: Pin memory for faster GPU transfer

    Returns:
        Tuple of (train_loader, val_loader)
    """
    data_dir = Path(data_dir)

    # Create datasets
    train_dataset = DrumDatasetCNN(data_dir / 'train_data_cnn.npz')
    val_dataset = DrumDatasetCNN(data_dir / 'val_data_cnn.npz')

    print()

    # Create samplers
    if use_weighted_sampling:
        print("Using weighted sampling for training")
        sample_weights = train_dataset.get_sample_weights()
        sampler = WeightedRandomSampler(
            weights=sample_weights,
            num_samples=len(sample_weights),
            replacement=True
        )
        shuffle = False
    else:
        sampler = None
        shuffle = True

    # Create dataloaders
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        sampler=sampler,
        num_workers=num_workers,
        pin_memory=pin_memory,
        drop_last=True  # Drop incomplete batch for BatchNorm
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=pin_memory
    )

    return train_loader, val_loader


if __name__ == '__main__':
    # Test dataset
    print("="*70)
    print("Testing DrumDatasetCNN")
    print("="*70)
    print()

    data_dir = '/mnt/gs21/scratch/meadowm1/music-ai-training/beatbox2drums/classifier_data_cnn'

    # Test loading datasets
    val_dataset = DrumDatasetCNN(data_dir + '/val_data_cnn.npz')

    print()

    # Test getting a sample
    spec, label = val_dataset[0]
    print(f"Sample shape: {spec.shape}")
    print(f"Label: {label} ({val_dataset.IDX_TO_CLASS[label]})")
    print(f"Spectrogram range: [{spec.min():.2f}, {spec.max():.2f}]")
    print()

    # Test class weights
    class_weights = val_dataset.get_class_weights()
    print("Class weights:")
    for i, name in enumerate(val_dataset.CLASS_NAMES):
        print(f"  {name}: {class_weights[i]:.4f}")
    print()

    print("="*70)
    print("✓ Dataset test successful!")
    print("="*70)
